{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 Synthetic Dataset Generation for Embedding Finetuning Tasks\n",
    "\n",
    "In this Jupyter notebook, we demonstrate how to leverage a Python script designed for generating a synthetic dataset of (query, relevant document) pairs from a corpus of documents that can be used to finetune embeddings models to improve performance in custom RAG and retrival AI use cases. We use natural language processing (NLP) techniques and a language model to automate the creation of a dataset suitable for tasks such as question answering, search, and information retrieval.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary components from our script. This involves loading the corpus, generating queries, and saving our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Corpus\n",
    "\n",
    "We begin by importing the relevant helper functions from the script, initializing our `CorpusLoader` with a directory containing our PDF documents. This class will load and split our corpus into training and validation sets. \n",
    "\n",
    "We create the corpus of text chunks by leveraging LlamaIndex to load some sample PDFs, and parsing/chunking into plain text chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "kit_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "repo_dir = os.path.abspath(os.path.join(kit_dir, \"..\"))\n",
    "\n",
    "sys.path.append(kit_dir)\n",
    "sys.path.append(repo_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tuning_embeddings.src.generate_fine_tune_embed_dataset import CorpusLoader, QueryGenerator, LangChainLLM, save_dict_safely\n",
    "from utils.model_wrappers.api_gateway import APIGateway\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.path.join(kit_dir, \"sample_data\")\n",
    "val_ratio = 0.2\n",
    "\n",
    "corpus_loader = CorpusLoader(directory=data_directory, val_ratio=val_ratio)\n",
    "train_corpus = corpus_loader.load_corpus(corpus_loader.train_files)\n",
    "val_corpus = corpus_loader.load_corpus(corpus_loader.val_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_loader.val_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Loaded Corpora\n",
    "\n",
    "After loading the training and validation corpora, we save them to files for later use. This ensures we can easily reload the corpora without reprocessing the original documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_output_path =  os.path.join(kit_dir, \"data/train_corpus.json\")\n",
    "val_corpus_output_path =  os.path.join(kit_dir, \"data/val_corpus.json\")\n",
    "\n",
    "corpus_loader.save_corpus(train_corpus, train_corpus_output_path)\n",
    "corpus_loader.save_corpus(val_corpus, val_corpus_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Language Model (LLM)\n",
    "\n",
    "For generating queries, we define the language model (LLM) to use. You can choose between a SambaNova model or an OpenAI / other LLM provider model based on your requirements and access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file: str) -> None:\n",
    "        \"\"\"\n",
    "        Load configuration parameters from a YAML file.\n",
    "\n",
    "        Parameters:\n",
    "        config_file (str): Path to the YAML configuration file.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        with open(config_file, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            return config\n",
    "\n",
    "config = load_config(os.path.join(kit_dir, 'config.yaml'))        \n",
    "llm_info = config['llm']\n",
    "\n",
    "llm = APIGateway.load_llm(\n",
    "            type=llm_info['api'],\n",
    "            streaming=True,\n",
    "            coe=llm_info['coe'],\n",
    "            do_sample=llm_info['do_sample'],\n",
    "            max_tokens_to_generate=llm_info['max_tokens_to_generate'],\n",
    "            temperature=llm_info['temperature'],\n",
    "            select_expert=llm_info['select_expert'],\n",
    "            process_prompt=False,\n",
    "        )\n",
    "\n",
    "# Convert SN Endpoint to LangChain LLM As The Wrapper Is In Langchain\n",
    "llm = LangChainLLM(llm=llm)\n",
    "\n",
    "\n",
    "# For OpenAI:\n",
    "# llm = OpenAI(model='gpt-3.5-turbo')  # This line remains commented in the script for instructional purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the QueryGenerator with your language model\n",
    "# Note: Ensure you have access to the LLM and its credentials\n",
    "# Note: Depending on the size of your corpus & model inference time, this can take a long time! \n",
    "\n",
    "query_generator = QueryGenerator(llm=llm)\n",
    "\n",
    "train_queries, train_relevant_docs = query_generator.generate_queries(train_corpus, verbose=True)\n",
    "val_queries, val_relevant_docs = query_generator.generate_queries(val_corpus, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Generated Queries\n",
    "\n",
    "It's essential to inspect the generated queries and their corresponding relevant documents to ensure the quality of our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display generated queries and documents\n",
    "def display_generated_data(queries, relevant_docs, corpus, num_samples=5):\n",
    "    sample_queries = random.sample(list(queries.items()), num_samples)\n",
    "    \n",
    "    for query_id, query in sample_queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        doc_ids = relevant_docs[query_id]\n",
    "        for doc_id in doc_ids:\n",
    "            print(f\"Relevant Document: {corpus[doc_id][:200]}...\")  # Display the first 200 characters\n",
    "        print(\"\\n\")\n",
    "\n",
    "display_generated_data(train_queries, train_relevant_docs, train_corpus)\n",
    "display_generated_data(val_queries, val_relevant_docs, val_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Dataset\n",
    "\n",
    "Finally, we save our generated dataset safely to ensure it can be used for training NLP models without running into memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_path =  os.path.join(kit_dir, \"data/train_dataset.json\")\n",
    "val_output_path =  os.path.join(kit_dir, \"data/val_dataset.json\")\n",
    "\n",
    "save_dict_safely({'queries': train_queries, 'corpus': train_corpus, 'relevant_docs': train_relevant_docs}, train_output_path)\n",
    "save_dict_safely({'queries': val_queries, 'corpus': val_corpus, 'relevant_docs': val_relevant_docs}, val_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive guide on generating a synthetic dataset for NLP tasks using Python. By automating the generation of queries and relevant documents, we streamline the process of creating rich datasets for training models on tasks such as question answering and information retrieval.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embeds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
